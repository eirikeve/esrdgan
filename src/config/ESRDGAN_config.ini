# This is the basic config.  name is used to create a folder in the root/experiments directory
[DEFAULT]
# Name of model. This is used for the folder name under the runs/ folder as well.
name                    = ESRDGAN_train_20_4x_LFF3_shallow_many_feat_no_gauss_gan
# Only one supported right now is esrdgan. :)
model                   = esrdgan
# Logs to esrdgan/tensorboard_log
use_tensorboard_logger  = True
# super resolution scale
scale                   = 4
# Remove the `= <num>` to use CPU 
gpu_id                  = 0
also_log_to_terminal    = True
# Set this to true to use the generator_load_path, discriminator_load_path, and state_load_path to load a model.
load_model_from_save    = False
# Progress bar
display_bar             = True

[ENV]
# Root dir of project.
root_path = /home/eirikeve/Programming/esrdgan
log_subpath  = /log
tensorboard_subpath = /tensorboard_log
runs_subpath = /runs
# If this has a value, and load_model_from_save = True, then G is loaded from this.
generator_load_path 
# If this has a value, and load_model_from_save = True, then D is loaded from this.
discriminator_load_path 
# If this has a value,  load_model_from_save = True, and resume_training_from_save = True then training is resumed from this.
state_load_path 

# There are three datasets: TRAIN, TEST, and VAL.
# TRAIN/VAL are used for --train
# TEST is used for --test and --use
[DATASETTRAIN]
# allowed modes are downsampler (creates LR during runtime), lr (for --use), and hrlr (for specifying your own LR/HR)
mode = downsampler
n_workers = 8
batch_size  = 8
# Only 128 is supported. Other dimensions would require a change in the discriminator.
hr_img_size = 128
# just for info
name  = Flickr2K
# if mode is lr or hrlr, then dataroot_lr must also be specified
dataroot_hr = /home/eirikeve/Programming/Datasets/Flickr2K/HR_256
# data augmentation: gaussian noise on the LR images
data_aug_gaussian_noise = False
gaussian_stddev = 0.00
data_aug_shuffle = True
data_aug_flip = True
# This option is not currently in use.
data_aug_rot = True

[DATASETTEST]
mode = lr
n_workers = 8
batch_size  = 1
hr_img_size = -1
name  = Set5
dataroot_hr
dataroot_lr = /home/eirikeve/Programming/Datasets/Set5/HR
data_aug_gaussian_noise = False
gaussian_stddev = 0.00
data_aug_shuffle = False
data_aug_flip = False
data_aug_rot = False

[DATASETVAL]
mode = downsampler
n_workers = 8
batch_size  = 1
hr_img_size = 128
name  = Set14
dataroot_hr = /home/eirikeve/Programming/Datasets/BSDS100/HR
data_aug_gaussian_noise = False
gaussian_stddev = 0.00
data_aug_shuffle = False
data_aug_flip = False
data_aug_rot = False

[GENERATOR]
norm_type           = none
act_type            = leakyrelu
layer_mode          = CNA
# base # of features extracted = # channels
num_features        = 128
# number of residual in residual dense blocks
num_rrdb            = 12
# this is not currently used
num_rdb_convs       = 5
rdb_res_scaling     = 0.2
rrdb_res_scaling    = 0.2
# RGB = 3
in_num_ch           = 3
out_num_ch          = 3
rdb_growth_chan     = 32
hr_kern_size        = 3
weight_init_scale   = 0.5
# lff = local feature fusion layer of the RDB
lff_kern_size       = 3


[DISCRIMINATOR]
norm_type       = batch
act_type        = leakyrelu
layer_mode      = CNA
num_features    = 96
in_num_ch       = 3
feat_kern_size  = 3
weight_init_scale   = 1.0


[FEATUREEXTRACTOR]
# If this is specified, low level features are also extracted.
# This did not work well when I used it. -1 disables it.
low_level_feat_layer = -1
high_level_feat_layer = 34

[TRAINING]
# See [ENV]
resume_training_from_save = False
learning_rate_g = 1e-4
learning_rate_d = 1e-4
adam_weight_decay_g = 0
adam_weight_decay_d = 0
adam_beta1_g = 0.9
adam_beta1_d = 0.9
# LR is decayed by factor lr_gamma for each entry in multistep_lr_steps if multistep_lr = True
multistep_lr = True
multistep_lr_steps = [10000, 20000, 30000, 40000]
lr_gamma = 0.5
# this specifies the GAN adversarial loss: relativistic or relativisticavg
gan_type = relativistic
gan_weight = 5e-3
# How often D is updated relative to G. 
d_g_train_ratio = 2
pixel_criterion = l1
pixel_weight = 0.01
feature_criterion = l1
feature_weight = 0.1
# add noise to the labels for D - with stddev 0.05
use_noisy_labels = False
# 1 -> 0.9 for the labels for D. Penalizes D if its predictions are too strong
use_one_sided_label_smoothing = True
# With: Flip labels for real and fake data for D.
flip_labels = False
# Instance noise is gaussian noise added on the input to D
# beginning at var 1, decreasing linearly to var 0 at training end.
use_instance_noise = True
# iterations to train for
niter  = 100000
val_period = 1000
# output images and models are saved
save_model_period  = 5000
# for fetching logs to the logfiles. Not important.
log_period = 10
