# esrdgan
GAN implementation for single image super resolution. Based on recent research.


## Summary of literature review


1) [SRGAN](http://openaccess.thecvf.com/content_cvpr_2017/papers/Ledig_Photo-Realistic_Single_Image_CVPR_2017_paper.pdf), [ESRGAN](https://arxiv.org/pdf/1809.00219.pdf), and [EnhanceNET](https://webdav.tue.mpg.de/pixel/enhancenet/files/EnhanceNet_paper.pdf) are three recent implementations that all give good results.
   1) ESRGAN authors have a github repository ([BasicSR](https://github.com/xinntao/BasicSR)) with code for building and training SRGAN, ESRGAN, etc. with various modifications.  
   2) SRGAN and EnhanceNET use Residual blocks (-> ResNET), whereas ESRGAN replaces it with a Residual in Residual Dense block. All of the networks share one important property: Shortcuts which ensure the gradient does not decay completely.  
2) I am planning on basing my implementation on ESRGAN, but with modifications (in italic). 
   1) Loss functions:
      1) Pixel space L1 or L2 loss
      2) Feature space L1 or L2 loss, using VGG-19, but at *both low and high level layers*, inspired by EnhanceNet
      3) Relativistic GAN loss like in ESRGAN.  ([J-M: The relativistic discriminator](https://arxiv.org/pdf/1807.00734.pdf)) This supposedly works even better than WGAN, and gives the same benefits (no mode collapse etc...).
   2) Architecture:  
      1) Possibly deeper than ESRGAN, which worked well for SRDGAN.
   3) Training:
      1) Pretrain without GAN loss for ~5 hours, and use that as the base model for all of my experiments.
      2) Train each experiment for a maximum of ~5 hours.
      3) Start with similar hyperparameters as ESRGAN.
         1) Loss type (L1/L2) etc. also as parameters.
      4) possibly try another optimizer than Adam, as I've read that it might not give the best solutions.
   4) Datasets:
      1) Base this on the very recent [SRDGAN](https://arxiv.org/pdf/1903.11821.pdf) paper. It highlights some possibly large issues in the standard procedure for generating LR images: Bicubic interpolation encodes a lot of information in the resulting image - more than one can expect in a normal phone photography etc., which means that most SR-implementations are good at SR on LR images generated by bicubic interpolation - but not as good on *normal* photos. Depending on the application, this can be good/bad.
      2) So: Two options:
         1) Train a NN to generate realistic looking low res images : `LR = HRtoLR( HR + normal(0, sigma1))` to use as the input
         2) Or use `LR = torch.functional.interpolate(HR, mode='nearest') + normal(0, sigma2)` as the input.
         3) In any case, the idea here is to use worse inputs, as it is a much more likely training scenario.
         4) I started on number 1 (made a model which trains!), but I think that might be harder than the upscaler actually: How to judge if the low res image is "good"/"realistic"? What to use as real data to the GAN?
3) I am considering several ways of evaluating them:


**Metrics**
- PSNR
- SSIM
- IFC
- These metrics might not give good scores for photo realistic images! Why is that? 
  - PSNR: Pixel average -> not good
- Usually, people are surveyed for their opinion.

- Object recognition quality benchmark
  - Feed imageNet through super-resolution models
  - Run pre-trained object detection network on results
  - *This seems like a good approach, but a bit more difficult.*

